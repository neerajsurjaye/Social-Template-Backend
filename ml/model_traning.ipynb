{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ask = pd.read_csv('./AskReddit.csv')\n",
    "dataset_jokes = pd.read_csv('./Jokes.csv')\n",
    "dataset_five = pd.read_csv('./explainlikeimfive.csv')\n",
    "dataset_life = pd.read_csv('./LifeProTips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_jokes['title'].values\n",
    "dataset = np.append(dataset , dataset_ask['title'].values)\n",
    "dataset = np.append(dataset , dataset_five['title'].values)\n",
    "dataset = np.append(dataset , dataset_life['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in range(0 , len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]' , ' ' , dataset[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    # print(review)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "\n",
    "    review = [ps.stem(x) for x in review]\n",
    "    review = ' '.join(review)\n",
    "    words.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq = set()\n",
    "\n",
    "# for x in words:\n",
    "    # for y in x.split():\n",
    "        # uniq.add(y)\n",
    "\n",
    "# print(len(uniq))\n",
    "# print(uniq)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "x = cv.fit_transform(words).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=15, random_state=57)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = KMeans(n_clusters=15 , init='k-means++' , random_state=57)\n",
    "cluster.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "post = \"\"\n",
    "post = re.sub('[^a-zA-Z]' , ' ' , post)\n",
    "post = post.lower()\n",
    "post = cv.transform([post]).toarray()\n",
    "cluster.predict(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./cluster_model' , 'wb') as file:\n",
    "    pickle.dump(cluster , file)\n",
    "\n",
    "with open('./vectorizer_trained' , 'wb') as file:\n",
    "    pickle.dump(cv , file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62006d1f3422635846181a997a61e8ec3049f797e5d7dfe0cd1bb84092b7c19"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
